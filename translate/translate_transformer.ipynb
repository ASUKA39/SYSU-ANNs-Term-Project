{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于Transformer的机器翻译\n",
    "\n",
    "机器翻译是利用计算机将一种自然语言(源语言)转换为另一种自然语言(目标语言)的过程。\n",
    "\n",
    "本项目是机器翻译领域主流模型 Transformer 的 PaddlePaddle 实现，包含模型训练，预测以及使用自定义数据等内容。用户可以基于发布的内容搭建自己的翻译模型。\n",
    "\n",
    "Transformer 是论文 [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf) 中提出的用以完成机器翻译（Machine Translation）等序列到序列（Seq2Seq）学习任务的一种全新网络结构，其完全使用注意力（Attention）机制来实现序列到序列的建模。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/6e48d8033d7b4b8a8a34baa3af7ebdab2d3d852f7e8348a58e462ed3089db668\" width=\"500\" height=\"313\" ></center>\n",
    "<br><center>图1：Transformer 网络结构图</center></br>\n",
    "\n",
    "相较于此前 Seq2Seq 模型中广泛使用的循环神经网络（Recurrent Neural Network, RNN），使用Self Attention进行输入序列到输出序列的变换主要具有以下优势：\n",
    "\n",
    "- 计算复杂度小\n",
    "\t- 特征维度为 d 、长度为 n 的序列，在 RNN 中计算复杂度为 O(n * d * d) （n 个时间步，每个时间步计算 d 维的矩阵向量乘法），在 Self-Attention 中计算复杂度为 O(n * n * d) （n 个时间步两两计算 d 维的向量点积或其他相关度函数），n 通常要小于 d 。\n",
    "- 计算并行度高\n",
    "\t- RNN 中当前时间步的计算要依赖前一个时间步的计算结果；Self-Attention 中各时间步的计算只依赖输入不依赖之前时间步输出，各时间步可以完全并行。\n",
    "- 容易学习长距离依赖（long-range dependencies）\n",
    "\t- RNN 中相距为 n 的两个位置间的关联需要 n 步才能建立；Self-Attention 中任何两个位置都直接相连；路径越短信号传播越容易。\n",
    "Transformer 中引入使用的基于 Self-Attention 的序列建模模块结构，已被广泛应用在 Bert 等语义表示模型中，取得了显著效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 环境介绍\n",
    "\n",
    "- PaddlePaddle框架，AI Studio平台已经默认安装最新版2.1。\n",
    "\n",
    "- PaddleNLP，深度兼容框架2.1，是飞桨框架2.1在NLP领域的最佳实践。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T06:34:09.322832Z",
     "iopub.status.busy": "2024-01-13T06:34:09.322389Z",
     "iopub.status.idle": "2024-01-13T06:34:11.505007Z",
     "shell.execute_reply": "2024-01-13T06:34:11.503663Z",
     "shell.execute_reply.started": "2024-01-13T06:34:09.322800Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  transformer_mt.zip\r\n",
      "   creating: transformer_mt/\r\n",
      "  inflating: transformer_mt/1918692.ipynb  \r\n",
      "  inflating: transformer_mt/get_data_and_model.sh  \r\n",
      " extracting: transformer_mt/mosesdecoder.tar.gz  \r\n",
      "  inflating: transformer_mt/preprocess.sh  \r\n",
      " extracting: transformer_mt/requirements.txt  \r\n",
      " extracting: transformer_mt/train_dev_test.tar.gz  \r\n",
      "  inflating: transformer_mt/transformer.base.yaml  \r\n",
      "  inflating: transformer_mt/utils.py  \r\n",
      "   creating: transformer_mt/__pycache__/\r\n",
      "  inflating: transformer_mt/__pycache__/utils.cpython-37.pyc  \r\n"
     ]
    }
   ],
   "source": [
    "!unzip -o transformer_mt.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-13T06:34:11.507903Z",
     "iopub.status.busy": "2024-01-13T06:34:11.507252Z",
     "iopub.status.idle": "2024-01-13T06:34:11.515948Z",
     "shell.execute_reply": "2024-01-13T06:34:11.514906Z",
     "shell.execute_reply.started": "2024-01-13T06:34:11.507858Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio/transformer_mt\r\n"
     ]
    }
   ],
   "source": [
    "%cd transformer_mt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-13T06:34:11.520593Z",
     "iopub.status.busy": "2024-01-13T06:34:11.519634Z",
     "iopub.status.idle": "2024-01-13T06:34:33.134216Z",
     "shell.execute_reply": "2024-01-13T06:34:33.133147Z",
     "shell.execute_reply.started": "2024-01-13T06:34:11.520536Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple, https://mirrors.aliyun.com/pypi/simple/, https://pypi.tuna.tsinghua.edu.cn/simple/\r\n",
      "Collecting paddlenlp==2.3.2\r\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/23/99/8bc858da7fa76b4e44de3907bbd045a22b1eb6a5c893ab87a521571d4c20/paddlenlp-2.3.2-py3-none-any.whl (1.4 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m935.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.2) (0.42.1)\r\n",
      "Requirement already satisfied: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.2) (0.4.4)\r\n",
      "Collecting datasets>=2.0.0\r\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/d3/95/ef83542e7a8e2bfc4432ee2cd8a6b52eb30fb1e605871e8871e94ce65fb1/datasets-2.13.2-py3-none-any.whl (512 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.7/512.7 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.2) (4.1.0)\r\n",
      "Requirement already satisfied: protobuf<=3.20.0,>=3.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.2) (3.20.0)\r\n",
      "Requirement already satisfied: multiprocess<=0.70.12.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.2) (0.70.11.1)\r\n",
      "Requirement already satisfied: dill<0.3.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.2) (0.3.3)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.2) (0.1.96)\r\n",
      "Requirement already satisfied: paddle2onnx in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.2) (1.0.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.2) (4.66.1)\r\n",
      "Requirement already satisfied: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.2) (1.2.2)\r\n",
      "Requirement already satisfied: paddlefsl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.2) (1.0.0)\r\n",
      "Collecting xxhash\r\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/54/a0/dae1c5dc27601a61897b48a367232c743c760c765d9ab38be1a903cf0d87/xxhash-3.4.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/194.6 kB\u001b[0m \u001b[31m577.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.2) (2.24.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.2) (5.1.2)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.2) (1.1.5)\r\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.2) (2023.1.0)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.2) (21.3)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.2) (4.2.0)\r\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.2) (12.0.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.2) (1.19.5)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.2) (3.8.6)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.2) (0.16.4)\r\n",
      "Collecting paddlefsl\r\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/fb/4a/25d1959a8f1fe5ee400f32fc9fc8b56d4fd6fc25315e23c0171f6e705e2a/paddlefsl-1.1.0-py3-none-any.whl (101 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.0/101.0 kB\u001b[0m \u001b[31m351.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp==2.3.2) (0.24.2)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.3.2) (22.1.0)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.3.2) (1.3.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.3.2) (4.7.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.3.2) (3.3.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.3.2) (6.0.4)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.3.2) (1.3.3)\r\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.3.2) (0.13.0)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.3.2) (4.0.3)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.3.2) (1.9.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets>=2.0.0->paddlenlp==2.3.2) (3.0.12)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from packaging->datasets>=2.0.0->paddlenlp==2.3.2) (3.0.9)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp==2.3.2) (3.0.4)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp==2.3.2) (1.25.6)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp==2.3.2) (2019.9.11)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp==2.3.2) (2.8)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.3.2) (2.1.0)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.3.2) (1.6.3)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.3.2) (0.14.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata->datasets>=2.0.0->paddlenlp==2.3.2) (3.8.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pandas->datasets>=2.0.0->paddlenlp==2.3.2) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pandas->datasets>=2.0.0->paddlenlp==2.3.2) (2019.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets>=2.0.0->paddlenlp==2.3.2) (1.16.0)\r\n",
      "Installing collected packages: xxhash, paddlefsl, datasets, paddlenlp\r\n",
      "  Attempting uninstall: paddlefsl\r\n",
      "    Found existing installation: paddlefsl 1.0.0\r\n",
      "    Uninstalling paddlefsl-1.0.0:\r\n",
      "      Successfully uninstalled paddlefsl-1.0.0\r\n",
      "  Attempting uninstall: paddlenlp\r\n",
      "    Found existing installation: paddlenlp 2.1.1\r\n",
      "    Uninstalling paddlenlp-2.1.1:\r\n",
      "      Successfully uninstalled paddlenlp-2.1.1\r\n",
      "Successfully installed datasets-2.13.2 paddlefsl-1.1.0 paddlenlp-2.3.2 xxhash-3.4.1\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple, https://mirrors.aliyun.com/pypi/simple/, https://pypi.tuna.tsinghua.edu.cn/simple/\r\n",
      "Collecting attrdict==2.0.1\r\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/ef/97/28fe7e68bc7adfce67d4339756e85e9fcf3c6fd7f0c0781695352b70472c/attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\r\n",
      "Collecting PyYAML==5.4.1\r\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m636.6/636.6 kB\u001b[0m \u001b[31m361.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hCollecting subword_nmt==0.3.7\r\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/74/60/6600a7bc09e7ab38bc53a48a20d8cae49b837f93f5842a41fe513a694912/subword_nmt-0.3.7-py2.py3-none-any.whl (26 kB)\r\n",
      "Requirement already satisfied: jieba==0.42.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (0.42.1)\r\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from attrdict==2.0.1->-r requirements.txt (line 1)) (1.16.0)\r\n",
      "Installing collected packages: subword_nmt, PyYAML, attrdict\r\n",
      "  Attempting uninstall: PyYAML\r\n",
      "    Found existing installation: PyYAML 5.1.2\r\n",
      "    Uninstalling PyYAML-5.1.2:\r\n",
      "      Successfully uninstalled PyYAML-5.1.2\r\n",
      "Successfully installed PyYAML-5.4.1 attrdict-2.0.1 subword_nmt-0.3.7\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# 安装依赖\n",
    "!pip install paddlenlp==2.3.2 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/b7aafbcc8e2f4bfc9b864d1a1bc0af749260ef9b690c458399f2ba9c66c1ab80\" width=\"1200\" height=\"600\" ></center>\n",
    "<br><center>图2：Pipeline </center></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-13T06:54:06.212689Z",
     "iopub.status.busy": "2024-01-13T06:54:06.208455Z",
     "iopub.status.idle": "2024-01-13T06:54:06.219000Z",
     "shell.execute_reply": "2024-01-13T06:54:06.218248Z",
     "shell.execute_reply.started": "2024-01-13T06:54:06.212645Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import yaml\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from attrdict import AttrDict\n",
    "import jieba\n",
    "\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "import paddle.distributed as dist\n",
    "from paddle.io import DataLoader,BatchSampler\n",
    "from paddlenlp.data import Vocab, Pad\n",
    "from paddlenlp.datasets import load_dataset\n",
    "# from paddlenlp.transformers import TransformerModel, InferTransformerModel, CrossEntropyCriterion, position_encoding_init\n",
    "from paddlenlp.transformers import *\n",
    "from paddlenlp.utils.log import logger\n",
    "\n",
    "from utils import post_process_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据预处理\n",
    "本教程使用[CWMT](http://nlp.nju.edu.cn/cwmt-wmt/)数据集中的中文英文的数据作为训练语料，\n",
    "CWMT数据集在900万+，质量较高，非常适合来训练Transformer机器翻译。  \n",
    "中文需要Jieba+BPE，英文需要BPE  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE(Byte Pair Encoding)\n",
    "BPE优势：\n",
    "- 压缩词表；\n",
    "- 一定程度上缓解OOV(out of vocabulary)问题\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/e7a59d7bab514a6fa17d24a116af0b680fbd664439c948799c0a0541dffd35a2\" width=\"1000\" height=\"500\" ></center>\n",
    "<br><center>图3：learn BPE </center></br>\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/d4b9c48ba7274395af3fbb267c1f9adcba50dd4b147d4258be58999b3b5a198c\" width=\"1000\" height=\"500\" ></center>\n",
    "\n",
    "<br><center>图4：Apply BPE </center></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/c319a24e5612413fb715885d7143f62882eba16ce43943c5b53903963591687c\" width=\"1000\" height=\"500\" ></center>\n",
    "<br><center>图5：Jieba+BPE </center></br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-13T06:34:36.479553Z",
     "iopub.status.busy": "2024-01-13T06:34:36.478667Z",
     "iopub.status.idle": "2024-01-13T06:34:46.369159Z",
     "shell.execute_reply": "2024-01-13T06:34:46.366566Z",
     "shell.execute_reply.started": "2024-01-13T06:34:36.479519Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompress train_dev_test data...\r\n",
      "jieba tokenize...\r\n",
      "Building prefix dict from the default dictionary ...\r\n",
      "Dumping model to file cache /tmp/jieba.cache\r\n",
      "Loading model cost 0.939 seconds.\r\n",
      "Prefix dict has been built successfully.\r\n",
      "source learn-bpe and apply-bpe...\r\n",
      "no pair has frequency >= 2. Stopping\r\n",
      "target learn-bpe and apply-bpe...\r\n",
      "no pair has frequency >= 2. Stopping\r\n",
      "source get-vocab. if loading pretrained model, use its vocab.\r\n",
      "target get-vocab. if loading pretrained model, use its vocab.\r\n",
      "Over.\r\n"
     ]
    }
   ],
   "source": [
    "# 数据预处理过程，包括jieba分词、bpe分词和词表。\n",
    "!bash preprocess.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-13T06:34:46.371756Z",
     "iopub.status.busy": "2024-01-13T06:34:46.371055Z",
     "iopub.status.idle": "2024-01-13T06:36:00.657952Z",
     "shell.execute_reply": "2024-01-13T06:36:00.656670Z",
     "shell.execute_reply.started": "2024-01-13T06:34:46.371707Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download model.\r\n",
      "--2024-01-13 14:34:46--  https://paddlenlp.bj.bcebos.com/models/transformers/transformer/CWMT2021_step_345000.tar.gz\r\n",
      "正在解析主机 paddlenlp.bj.bcebos.com (paddlenlp.bj.bcebos.com)... 182.61.200.229, 182.61.200.195, 2409:8c04:1001:1002:0:ff:b001:368a\r\n",
      "正在连接 paddlenlp.bj.bcebos.com (paddlenlp.bj.bcebos.com)|182.61.200.229|:443... 已连接。\r\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\r\n",
      "长度： 1386250752 (1.3G) [application/x-gzip]\r\n",
      "正在保存至: “trained_models/CWMT2021_step_345000.tar.gz”\r\n",
      "\r\n",
      "CWMT2021_step_34500 100%[===================>]   1.29G  30.1MB/s    in 42s     \r\n",
      "\r\n",
      "2024-01-13 14:35:28 (31.5 MB/s) - 已保存 “trained_models/CWMT2021_step_345000.tar.gz” [1386250752/1386250752])\r\n",
      "\r\n",
      "Decompress model.\r\n",
      "Over.\r\n"
     ]
    }
   ],
   "source": [
    "# 下载预训练模型\n",
    "!bash get_data_and_model.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 构造Dataloader\n",
    "\n",
    "下面的`create_data_loader`函数用于创建训练集、验证集所需要的`DataLoader`对象,  \n",
    "`create_infer_loader`函数用于创建预测集所需要的`DataLoader`对象，   \n",
    "`DataLoader`对象用于产生一个个batch的数据。下面对函数中调用的`paddlenlp`内置函数作简单说明：\n",
    "* `paddlenlp.data.Vocab.load_vocabulary`：Vocab词表类，集合了一系列文本token与ids之间映射的一系列方法，支持从文件、字典、json等一系方式构建词表\n",
    "* `paddlenlp.datasets.load_dataset`：从本地文件创建数据集时，推荐根据本地数据集的格式给出读取function并传入 load_dataset() 中创建数据集\n",
    "* `paddlenlp.data.Pad`：padding 操作\n",
    "具体可参考[PaddleNLP的文档](https://paddlenlp.readthedocs.io/zh/latest/data_prepare/dataset_self_defined.html)\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/0085d53068134546bcb914347774430c3a2c94cd77934c2e90420ac740d16fc7\" width=\"700\" height=\"350\" ></center>\n",
    "<br><center>图6：构造Dataloader的流程 </center></br>\n",
    "\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/3bf38365718346a19c25729bb67e2e6afe8f0bafc61348018d2b9dd60dc9a8bf\" width=\"1000\" height=\"500\" ></center>\n",
    "<br><center>图7：Dataloader细节 </center></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-13T06:50:44.096383Z",
     "iopub.status.busy": "2024-01-13T06:50:44.095390Z",
     "iopub.status.idle": "2024-01-13T06:50:44.104633Z",
     "shell.execute_reply": "2024-01-13T06:50:44.103722Z",
     "shell.execute_reply.started": "2024-01-13T06:50:44.096325Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 自定义读取本地数据的方法\n",
    "def read(src_path, tgt_path, is_predict=False):\n",
    "    if is_predict:\n",
    "        with open(src_path, 'r', encoding='utf8') as src_f:\n",
    "            for src_line in src_f.readlines():\n",
    "                src_line = src_line.strip()\n",
    "                if not src_line:\n",
    "                    continue\n",
    "                yield {'src':src_line, 'tgt':''}\n",
    "    else:\n",
    "        with open(src_path, 'r', encoding='utf8') as src_f, open(tgt_path, 'r', encoding='utf8') as tgt_f:\n",
    "            for src_line, tgt_line in zip(src_f.readlines(), tgt_f.readlines()):\n",
    "                src_line = src_line.strip()\n",
    "                if not src_line:\n",
    "                    continue\n",
    "                tgt_line = tgt_line.strip()\n",
    "                if not tgt_line:\n",
    "                    continue\n",
    "                yield {'src':src_line, 'tgt':tgt_line}\n",
    " # 过滤掉长度 ≤min_len或者≥max_len 的数据            \n",
    "def min_max_filer(data, max_len, min_len=0):\n",
    "    # 1 for special tokens.\n",
    "    data_min_len = min(len(data[0]), len(data[1])) + 1\n",
    "    data_max_len = max(len(data[0]), len(data[1])) + 1\n",
    "    return (data_min_len >= min_len) and (data_max_len <= max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-13T06:50:46.587804Z",
     "iopub.status.busy": "2024-01-13T06:50:46.586913Z",
     "iopub.status.idle": "2024-01-13T06:50:46.600409Z",
     "shell.execute_reply": "2024-01-13T06:50:46.599406Z",
     "shell.execute_reply.started": "2024-01-13T06:50:46.587748Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 创建训练集、验证集的dataloader\n",
    "def create_data_loader(args):\n",
    "    train_dataset = load_dataset(read, src_path=args.training_file.split(',')[0], tgt_path=args.training_file.split(',')[1], lazy=False)\n",
    "    dev_dataset = load_dataset(read, src_path=args.validation_file.split(',')[0], tgt_path=args.validation_file.split(',')[1], lazy=False)\n",
    "\n",
    "    src_vocab = Vocab.load_vocabulary(\n",
    "        args.src_vocab_fpath,\n",
    "        bos_token=args.special_token[0],\n",
    "        eos_token=args.special_token[1],\n",
    "        unk_token=args.special_token[2])\n",
    "    trg_vocab = Vocab.load_vocabulary(\n",
    "        args.trg_vocab_fpath,\n",
    "        bos_token=args.special_token[0],\n",
    "        eos_token=args.special_token[1],\n",
    "        unk_token=args.special_token[2])\n",
    "\n",
    "    padding_vocab = (\n",
    "        lambda x: (x + args.pad_factor - 1) // args.pad_factor * args.pad_factor\n",
    "    )\n",
    "    args.src_vocab_size = padding_vocab(len(src_vocab))\n",
    "    args.trg_vocab_size = padding_vocab(len(trg_vocab))\n",
    "\n",
    "    def convert_samples(sample):\n",
    "        source = sample['src'].split()\n",
    "        target = sample['tgt'].split()\n",
    "\n",
    "        source = src_vocab.to_indices(source)\n",
    "        target = trg_vocab.to_indices(target)\n",
    "\n",
    "        return source, target\n",
    "\n",
    "    # 训练集dataloader和验证集dataloader\n",
    "    data_loaders = []\n",
    "    for i, dataset in enumerate([train_dataset, dev_dataset]):\n",
    "        dataset = dataset.map(convert_samples, lazy=False).filter(\n",
    "            partial(min_max_filer, max_len=args.max_length))\n",
    "\n",
    "        # BatchSampler: https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/BatchSampler_cn.html\n",
    "        batch_sampler = BatchSampler(dataset,batch_size=args.batch_size, shuffle=True,drop_last=False)\n",
    "        \n",
    "        # DataLoader: https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/DataLoader_cn.html\n",
    "        data_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_sampler=batch_sampler,\n",
    "            collate_fn=partial(\n",
    "                prepare_train_input,\n",
    "                bos_idx=args.bos_idx,\n",
    "                eos_idx=args.eos_idx,\n",
    "                pad_idx=args.bos_idx),\n",
    "                num_workers=0,\n",
    "                return_list=True)\n",
    "        data_loaders.append(data_loader)\n",
    "\n",
    "    return data_loaders\n",
    "\n",
    "\n",
    "def prepare_train_input(insts, bos_idx, eos_idx, pad_idx):\n",
    "    \"\"\"\n",
    "    Put all padded data needed by training into a list.\n",
    "    \"\"\"\n",
    "    word_pad = Pad(pad_idx)\n",
    "    src_word = word_pad([inst[0] + [eos_idx] for inst in insts])\n",
    "    trg_word = word_pad([[bos_idx] + inst[1] for inst in insts])\n",
    "    lbl_word = np.expand_dims(\n",
    "        word_pad([inst[1] + [eos_idx] for inst in insts]), axis=2)\n",
    "\n",
    "    data_inputs = [src_word, trg_word, lbl_word]\n",
    "\n",
    "    return data_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-13T06:50:49.322161Z",
     "iopub.status.busy": "2024-01-13T06:50:49.321693Z",
     "iopub.status.idle": "2024-01-13T06:50:49.332910Z",
     "shell.execute_reply": "2024-01-13T06:50:49.332044Z",
     "shell.execute_reply.started": "2024-01-13T06:50:49.322117Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 创建测试集的dataloader，原理步骤同上（创建训练集、验证集的dataloader）\n",
    "def create_infer_loader(args):\n",
    "    dataset = load_dataset(read, src_path=args.predict_file, tgt_path=None, is_predict=True, lazy=False)\n",
    "\n",
    "    src_vocab = Vocab.load_vocabulary(\n",
    "        args.src_vocab_fpath,\n",
    "        bos_token=args.special_token[0],\n",
    "        eos_token=args.special_token[1],\n",
    "        unk_token=args.special_token[2])\n",
    "    trg_vocab = Vocab.load_vocabulary(\n",
    "        args.trg_vocab_fpath,\n",
    "        bos_token=args.special_token[0],\n",
    "        eos_token=args.special_token[1],\n",
    "        unk_token=args.special_token[2])\n",
    "\n",
    "    padding_vocab = (\n",
    "        lambda x: (x + args.pad_factor - 1) // args.pad_factor * args.pad_factor\n",
    "    )\n",
    "    args.src_vocab_size = padding_vocab(len(src_vocab))\n",
    "    args.trg_vocab_size = padding_vocab(len(trg_vocab))\n",
    "\n",
    "    def convert_samples(sample):\n",
    "        source = sample['src'].split()\n",
    "        target = sample['tgt'].split()\n",
    "\n",
    "        source = src_vocab.to_indices(source)\n",
    "        target = trg_vocab.to_indices(target)\n",
    "\n",
    "        return source, target\n",
    "\n",
    "    dataset = dataset.map(convert_samples, lazy=False)\n",
    "\n",
    "    # BatchSampler: https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/BatchSampler_cn.html\n",
    "    batch_sampler = BatchSampler(dataset,batch_size=args.infer_batch_size,drop_last=False)\n",
    "    \n",
    "    # DataLoader: https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/DataLoader_cn.html\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=partial(\n",
    "            prepare_infer_input,\n",
    "            bos_idx=args.bos_idx,\n",
    "            eos_idx=args.eos_idx,\n",
    "            pad_idx=args.bos_idx),\n",
    "            num_workers=0,\n",
    "            return_list=True)\n",
    "    return data_loader, trg_vocab.to_tokens\n",
    "\n",
    "def prepare_infer_input(insts, bos_idx, eos_idx, pad_idx):\n",
    "    \"\"\"\n",
    "    Put all padded data needed by beam search decoder into a list.\n",
    "    \"\"\"\n",
    "    word_pad = Pad(pad_idx)\n",
    "    src_word = word_pad([inst[0] + [eos_idx] for inst in insts])\n",
    "\n",
    "    return [src_word, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 搭建模型\n",
    "PaddleNLP提供Transformer API供调用：\n",
    "* [`paddlenlp.transformers.TransformerModel`](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/paddlenlp/transformers/transformer/modeling.py#L523)：Transformer模型的实现\n",
    "* [`paddlenlp.transformers.InferTransformerModel`](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/paddlenlp/transformers/transformer/modeling.py#L702)：Transformer模型用于生成\n",
    "* [`paddlenlp.transformers.CrossEntropyCriterion`](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/paddlenlp/transformers/transformer/modeling.py#L191)：计算交叉熵损失\n",
    "* [`paddlenlp.transformers.position_encoding_init`](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/paddlenlp/transformers/transformer/modeling.py#L17)：Transformer 位置编码的初始化\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/7fdcad8a336d41b3a3b461de2adce5ae5b28317e681b4efdab29f53641866897\" width=\"500\" height=\"250\" ></center>\n",
    "<br><center>图8：模型搭建 </center></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/fb181b57c2d347b884502d5d11d8c61e918ee803069d4e26bcf4c6533cf948c6\" width=\"1000\" height=\"500\" ></center>\n",
    "<br><center>图9：Example </center></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "搭建Transformer网络，这里参考paddlenlp内定义的TransformerModel类，并使用paddlenlp提供的API\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/6e48d8033d7b4b8a8a34baa3af7ebdab2d3d852f7e8348a58e462ed3089db668\" width=\"500\" height=\"313\" ></center>\n",
    "<br><center>图1：Transformer 网络结构图</center></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T06:53:37.114685Z",
     "iopub.status.busy": "2024-01-13T06:53:37.113999Z",
     "iopub.status.idle": "2024-01-13T06:53:37.132363Z",
     "shell.execute_reply": "2024-01-13T06:53:37.131454Z",
     "shell.execute_reply.started": "2024-01-13T06:53:37.114649Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Layer):\r\n",
    "    def __init__(\r\n",
    "        self,\r\n",
    "        src_vocab_size,\r\n",
    "        trg_vocab_size,\r\n",
    "        max_length,\r\n",
    "        num_encoder_layers,\r\n",
    "        num_decoder_layers,\r\n",
    "        n_head,\r\n",
    "        d_model,\r\n",
    "        d_inner_hid,\r\n",
    "        dropout,\r\n",
    "        weight_sharing,\r\n",
    "        attn_dropout=None,\r\n",
    "        act_dropout=None,\r\n",
    "        bos_id=0,\r\n",
    "        eos_id=1,\r\n",
    "        pad_id=None,\r\n",
    "        activation=\"relu\",\r\n",
    "        normalize_before=True,\r\n",
    "    ):\r\n",
    "        super(Transformer, self).__init__()\r\n",
    "        self.trg_vocab_size = trg_vocab_size\r\n",
    "        self.emb_dim = d_model\r\n",
    "        self.bos_id = bos_id\r\n",
    "        self.eos_id = eos_id\r\n",
    "        self.pad_id = pad_id if pad_id is not None else self.bos_id\r\n",
    "        self.dropout = dropout\r\n",
    "\r\n",
    "        self.src_word_embedding = WordEmbedding(vocab_size=src_vocab_size, emb_dim=d_model, bos_id=self.pad_id)\r\n",
    "        self.src_pos_embedding = PositionalEmbedding(emb_dim=d_model, max_length=max_length)\r\n",
    "        if weight_sharing:\r\n",
    "            assert (\r\n",
    "                src_vocab_size == trg_vocab_size\r\n",
    "            ), \"Vocabularies in source and target should be same for weight sharing.\"\r\n",
    "            self.trg_word_embedding = self.src_word_embedding\r\n",
    "            self.trg_pos_embedding = self.src_pos_embedding\r\n",
    "        else:\r\n",
    "            self.trg_word_embedding = WordEmbedding(vocab_size=trg_vocab_size, emb_dim=d_model, bos_id=self.pad_id)\r\n",
    "            self.trg_pos_embedding = PositionalEmbedding(emb_dim=d_model, max_length=max_length)\r\n",
    "\r\n",
    "        if not normalize_before:\r\n",
    "            encoder_layer = TransformerEncoderLayer(\r\n",
    "                d_model=d_model,\r\n",
    "                nhead=n_head,\r\n",
    "                dim_feedforward=d_inner_hid,\r\n",
    "                dropout=dropout,\r\n",
    "                activation=activation,\r\n",
    "                attn_dropout=attn_dropout,\r\n",
    "                act_dropout=act_dropout,\r\n",
    "                normalize_before=normalize_before,\r\n",
    "            )\r\n",
    "            encoder_with_post_norm = TransformerEncoder(encoder_layer, num_encoder_layers)\r\n",
    "\r\n",
    "            decoder_layer = TransformerDecoderLayer(\r\n",
    "                d_model=d_model,\r\n",
    "                nhead=n_head,\r\n",
    "                dim_feedforward=d_inner_hid,\r\n",
    "                dropout=dropout,\r\n",
    "                activation=activation,\r\n",
    "                attn_dropout=attn_dropout,\r\n",
    "                act_dropout=act_dropout,\r\n",
    "                normalize_before=normalize_before,\r\n",
    "            )\r\n",
    "            decoder_with_post_norm = TransformerDecoder(decoder_layer, num_decoder_layers)\r\n",
    "\r\n",
    "        self.transformer = paddle.nn.Transformer(\r\n",
    "            d_model=d_model,\r\n",
    "            nhead=n_head,\r\n",
    "            num_encoder_layers=num_encoder_layers,\r\n",
    "            num_decoder_layers=num_decoder_layers,\r\n",
    "            dim_feedforward=d_inner_hid,\r\n",
    "            dropout=dropout,\r\n",
    "            attn_dropout=attn_dropout,\r\n",
    "            act_dropout=act_dropout,\r\n",
    "            activation=activation,\r\n",
    "            normalize_before=normalize_before,\r\n",
    "            custom_encoder=None if normalize_before else encoder_with_post_norm,\r\n",
    "            custom_decoder=None if normalize_before else decoder_with_post_norm,\r\n",
    "        )\r\n",
    "\r\n",
    "        if weight_sharing:\r\n",
    "            self.linear = lambda x: paddle.matmul(\r\n",
    "                x=x, y=self.trg_word_embedding.word_embedding.weight, transpose_y=True\r\n",
    "            )\r\n",
    "        else:\r\n",
    "            self.linear = nn.Linear(in_features=d_model, out_features=trg_vocab_size, bias_attr=False)\r\n",
    "\r\n",
    "    def forward(self, src_word, trg_word):\r\n",
    "        src_max_len = paddle.shape(src_word)[-1]\r\n",
    "        trg_max_len = paddle.shape(trg_word)[-1]\r\n",
    "        src_slf_attn_bias = (\r\n",
    "            paddle.cast(src_word == self.pad_id, dtype=paddle.get_default_dtype()).unsqueeze([1, 2]) * -1e4\r\n",
    "        )\r\n",
    "        src_slf_attn_bias.stop_gradient = True\r\n",
    "        trg_slf_attn_bias = self.transformer.generate_square_subsequent_mask(trg_max_len)\r\n",
    "        trg_slf_attn_bias.stop_gradient = True\r\n",
    "        trg_src_attn_bias = src_slf_attn_bias\r\n",
    "        src_pos = paddle.cast(src_word != self.pad_id, dtype=src_word.dtype) * paddle.arange(\r\n",
    "            start=0, end=src_max_len, dtype=src_word.dtype\r\n",
    "        )\r\n",
    "        trg_pos = paddle.cast(trg_word != self.pad_id, dtype=src_word.dtype) * paddle.arange(\r\n",
    "            start=0, end=trg_max_len, dtype=trg_word.dtype\r\n",
    "        )\r\n",
    "\r\n",
    "        with paddle.static.amp.fp16_guard():\r\n",
    "            src_emb = self.src_word_embedding(src_word)\r\n",
    "            src_pos_emb = self.src_pos_embedding(src_pos)\r\n",
    "            src_emb = src_emb + src_pos_emb\r\n",
    "            enc_input = F.dropout(src_emb, p=self.dropout, training=self.training) if self.dropout else src_emb\r\n",
    "\r\n",
    "            trg_emb = self.trg_word_embedding(trg_word)\r\n",
    "            trg_pos_emb = self.trg_pos_embedding(trg_pos)\r\n",
    "            trg_emb = trg_emb + trg_pos_emb\r\n",
    "            dec_input = F.dropout(trg_emb, p=self.dropout, training=self.training) if self.dropout else trg_emb\r\n",
    "\r\n",
    "            dec_output = self.transformer(\r\n",
    "                enc_input,\r\n",
    "                dec_input,\r\n",
    "                src_mask=src_slf_attn_bias,\r\n",
    "                tgt_mask=trg_slf_attn_bias,\r\n",
    "                memory_mask=trg_src_attn_bias,\r\n",
    "            )\r\n",
    "\r\n",
    "            predict = self.linear(dec_output)\r\n",
    "\r\n",
    "        return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "搭建InferTransformer网络，用于生成，同样使用paddlenlp提供的API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T07:19:43.807536Z",
     "iopub.status.busy": "2024-01-13T07:19:43.806715Z",
     "iopub.status.idle": "2024-01-13T07:19:43.824203Z",
     "shell.execute_reply": "2024-01-13T07:19:43.823195Z",
     "shell.execute_reply.started": "2024-01-13T07:19:43.807496Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class InferTransformer(Transformer):\r\n",
    "    def __init__(\r\n",
    "        self,\r\n",
    "        src_vocab_size,\r\n",
    "        trg_vocab_size,\r\n",
    "        max_length,\r\n",
    "        num_encoder_layers,\r\n",
    "        num_decoder_layers,\r\n",
    "        n_head,\r\n",
    "        d_model,\r\n",
    "        d_inner_hid,\r\n",
    "        dropout,\r\n",
    "        weight_sharing,\r\n",
    "        attn_dropout=None,\r\n",
    "        act_dropout=None,\r\n",
    "        bos_id=0,\r\n",
    "        eos_id=1,\r\n",
    "        pad_id=None,\r\n",
    "        beam_size=4,\r\n",
    "        max_out_len=256,\r\n",
    "        output_time_major=False,\r\n",
    "        beam_search_version=\"v1\",\r\n",
    "        activation=\"relu\",\r\n",
    "        normalize_before=True,\r\n",
    "        **kwargs\r\n",
    "    ):\r\n",
    "        args = dict(locals())\r\n",
    "        args.pop(\"self\")\r\n",
    "        args.pop(\"__class__\", None)\r\n",
    "        self.beam_size = args.pop(\"beam_size\")\r\n",
    "        self.max_out_len = args.pop(\"max_out_len\")\r\n",
    "        self.output_time_major = args.pop(\"output_time_major\")\r\n",
    "        self.dropout = dropout\r\n",
    "        self.beam_search_version = args.pop(\"beam_search_version\")\r\n",
    "        kwargs = args.pop(\"kwargs\")\r\n",
    "        if self.beam_search_version == \"v2\":\r\n",
    "            self.alpha = kwargs.get(\"alpha\", 0.6)\r\n",
    "            self.rel_len = kwargs.get(\"rel_len\", False)\r\n",
    "        super(InferTransformer, self).__init__(**args)\r\n",
    "\r\n",
    "        cell = TransformerDecodeCell(\r\n",
    "            self.transformer.decoder, self.trg_word_embedding, self.trg_pos_embedding, self.linear, self.dropout\r\n",
    "        )\r\n",
    "\r\n",
    "        self.decode = TransformerBeamSearchDecoder(cell, bos_id, eos_id, beam_size, var_dim_in_state=2)\r\n",
    "\r\n",
    "    def forward(self, src_word, trg_word=None):\r\n",
    "        if trg_word is not None:\r\n",
    "            trg_length = paddle.sum(paddle.cast(trg_word != self.pad_id, dtype=\"int32\"), axis=-1)\r\n",
    "        else:\r\n",
    "            trg_length = None\r\n",
    "\r\n",
    "        if self.beam_search_version == \"v1\":\r\n",
    "            src_max_len = paddle.shape(src_word)[-1]\r\n",
    "            src_slf_attn_bias = (\r\n",
    "                paddle.cast(src_word == self.pad_id, dtype=paddle.get_default_dtype()).unsqueeze([1, 2]) * -1e4\r\n",
    "            )\r\n",
    "            trg_src_attn_bias = src_slf_attn_bias\r\n",
    "            src_pos = paddle.cast(src_word != self.pad_id, dtype=src_word.dtype) * paddle.arange(\r\n",
    "                start=0, end=src_max_len, dtype=src_word.dtype\r\n",
    "            )\r\n",
    "\r\n",
    "            # Run encoder\r\n",
    "            src_emb = self.src_word_embedding(src_word)\r\n",
    "            src_pos_emb = self.src_pos_embedding(src_pos)\r\n",
    "            src_emb = src_emb + src_pos_emb\r\n",
    "            enc_input = F.dropout(src_emb, p=self.dropout, training=False) if self.dropout else src_emb\r\n",
    "            enc_output = self.transformer.encoder(enc_input, src_slf_attn_bias)\r\n",
    "\r\n",
    "            # Init states (caches) for transformer, need to be updated according to selected beam\r\n",
    "            incremental_cache, static_cache = self.transformer.decoder.gen_cache(enc_output, do_zip=True)\r\n",
    "\r\n",
    "            static_cache, enc_output, trg_src_attn_bias = TransformerBeamSearchDecoder.tile_beam_merge_with_batch(\r\n",
    "                (static_cache, enc_output, trg_src_attn_bias), self.beam_size\r\n",
    "            )\r\n",
    "\r\n",
    "            rs, _ = nn.decode.dynamic_decode(\r\n",
    "                decoder=self.decode,\r\n",
    "                inits=incremental_cache,\r\n",
    "                max_step_num=self.max_out_len,\r\n",
    "                memory=enc_output,\r\n",
    "                trg_src_attn_bias=trg_src_attn_bias,\r\n",
    "                static_cache=static_cache,\r\n",
    "                is_test=True,\r\n",
    "                output_time_major=self.output_time_major,\r\n",
    "                trg_word=trg_word,\r\n",
    "                trg_length=trg_length,\r\n",
    "            )\r\n",
    "\r\n",
    "            return rs\r\n",
    "\r\n",
    "        elif self.beam_search_version == \"v2\":\r\n",
    "            finished_seq, finished_scores = self.beam_search_v2(\r\n",
    "                src_word, self.beam_size, self.max_out_len, self.alpha, trg_word, trg_length\r\n",
    "            )\r\n",
    "            if self.output_time_major:\r\n",
    "                finished_seq = finished_seq.transpose([2, 0, 1])\r\n",
    "            else:\r\n",
    "                finished_seq = finished_seq.transpose([0, 2, 1])\r\n",
    "\r\n",
    "            return finished_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.训练模型\n",
    "运行`do_train`函数，\n",
    "在`do_train`函数中，配置优化器、损失函数，以及评价指标Perplexity；  \n",
    "\n",
    "Perplexity，即困惑度，常用来衡量语言模型优劣，即句子的通顺度，一般用于机器翻译和文本生成等领域。Perplexity越小，句子越通顺，该语言模型越好。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/d6e0d38ae1d94deea1cc299a785b96663317a33d38e84f208c528c4dd03e83f2\" width=\"600\" height=\"300\" ></center>\n",
    "<br><center>图10：训练模型 </center></br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-13T06:53:40.691121Z",
     "iopub.status.busy": "2024-01-13T06:53:40.690299Z",
     "iopub.status.idle": "2024-01-13T06:53:40.707166Z",
     "shell.execute_reply": "2024-01-13T06:53:40.706350Z",
     "shell.execute_reply.started": "2024-01-13T06:53:40.691070Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def do_train(args):\n",
    "    if args.use_gpu:\n",
    "        place = \"gpu\"\n",
    "    else:\n",
    "        place = \"cpu\"\n",
    "    paddle.set_device(place)\n",
    "    # Set seed for CE\n",
    "    random_seed = eval(str(args.random_seed))\n",
    "    if random_seed is not None:\n",
    "        paddle.seed(random_seed)\n",
    "\n",
    "    # Define data loader\n",
    "    (train_loader), (eval_loader) = create_data_loader(args)\n",
    "\n",
    "    # Define model\n",
    "    transformer = Transformer(\n",
    "        src_vocab_size=args.src_vocab_size,\n",
    "        trg_vocab_size=args.trg_vocab_size,\n",
    "        max_length=args.max_length + 1,\n",
    "        num_encoder_layers=args.n_layer,\n",
    "        num_decoder_layers=args.n_layer,\n",
    "        n_head=args.n_head,\n",
    "        d_model=args.d_model,\n",
    "        d_inner_hid=args.d_inner_hid,\n",
    "        dropout=args.dropout,\n",
    "        weight_sharing=args.weight_sharing,\n",
    "        bos_id=args.bos_idx,\n",
    "        eos_id=args.eos_idx)\n",
    "\n",
    "    # Define loss\n",
    "    criterion = CrossEntropyCriterion(args.label_smooth_eps, args.bos_idx)\n",
    "\n",
    "    scheduler = paddle.optimizer.lr.NoamDecay(\n",
    "        args.d_model, args.warmup_steps, args.learning_rate, last_epoch=0)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = paddle.optimizer.Adam(\n",
    "        learning_rate=scheduler,\n",
    "        beta1=args.beta1,\n",
    "        beta2=args.beta2,\n",
    "        epsilon=float(args.eps),\n",
    "        parameters=transformer.parameters())\n",
    "\n",
    "    step_idx = 0\n",
    "\n",
    "    # Train loop\n",
    "    for pass_id in range(args.epoch):\n",
    "        batch_id = 0\n",
    "        for input_data in train_loader:\n",
    "\n",
    "            (src_word, trg_word, lbl_word) = input_data\n",
    "\n",
    "            logits = transformer(src_word=src_word, trg_word=trg_word)\n",
    "\n",
    "            sum_cost, avg_cost, token_num = criterion(logits, lbl_word)\n",
    "            \n",
    "            # 计算梯度\n",
    "            avg_cost.backward() \n",
    "            # 更新参数\n",
    "            optimizer.step() \n",
    "            # 梯度清零\n",
    "            optimizer.clear_grad() \n",
    "\n",
    "            if (step_idx + 1) % args.print_step == 0 or step_idx == 0:\n",
    "                total_avg_cost = avg_cost.numpy()\n",
    "                logger.info(\n",
    "                    \"step_idx: %d, epoch: %d, batch: %d, avg loss: %f, \"\n",
    "                    \" ppl: %f \" %\n",
    "                    (step_idx, pass_id, batch_id, total_avg_cost,\n",
    "                        np.exp([min(total_avg_cost, 100)])))\n",
    "\n",
    "            if (step_idx + 1) % args.save_step == 0:\n",
    "                # Validation\n",
    "                transformer.eval()\n",
    "                total_sum_cost = 0\n",
    "                total_token_num = 0\n",
    "                with paddle.no_grad():\n",
    "                    for input_data in eval_loader:\n",
    "                        (src_word, trg_word, lbl_word) = input_data\n",
    "                        logits = transformer(\n",
    "                            src_word=src_word, trg_word=trg_word)\n",
    "                        sum_cost, avg_cost, token_num = criterion(logits,\n",
    "                                                                  lbl_word)\n",
    "                        total_sum_cost += sum_cost.numpy()\n",
    "                        total_token_num += token_num.numpy()\n",
    "                        total_avg_cost = total_sum_cost / total_token_num\n",
    "                    logger.info(\"validation, step_idx: %d, avg loss: %f, \"\n",
    "                                \" ppl: %f\" %\n",
    "                                (step_idx, total_avg_cost,\n",
    "                                 np.exp([min(total_avg_cost, 100)])))\n",
    "                transformer.train()\n",
    "\n",
    "                if args.save_model:\n",
    "                    model_dir = os.path.join(args.save_model,\n",
    "                                             \"step_\" + str(step_idx))\n",
    "                    if not os.path.exists(model_dir):\n",
    "                        os.makedirs(model_dir)\n",
    "                    paddle.save(transformer.state_dict(),\n",
    "                                os.path.join(model_dir, \"transformer.pdparams\"))\n",
    "                    paddle.save(optimizer.state_dict(),\n",
    "                                os.path.join(model_dir, \"transformer.pdopt\"))\n",
    "            batch_id += 1\n",
    "            step_idx += 1\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "    if args.save_model:\n",
    "        model_dir = os.path.join(args.save_model, \"step_final\")\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "        paddle.save(transformer.state_dict(),\n",
    "                    os.path.join(model_dir, \"transformer.pdparams\"))\n",
    "        paddle.save(optimizer.state_dict(),\n",
    "                    os.path.join(model_dir, \"transformer.pdopt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-13T06:53:43.421781Z",
     "iopub.status.busy": "2024-01-13T06:53:43.421133Z",
     "iopub.status.idle": "2024-01-13T06:53:43.731461Z",
     "shell.execute_reply": "2024-01-13T06:53:43.730379Z",
     "shell.execute_reply.started": "2024-01-13T06:53:43.421743Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 50,\r\n",
      " 'beam_size': 5,\r\n",
      " 'beta1': 0.9,\r\n",
      " 'beta2': 0.997,\r\n",
      " 'bos_idx': 0,\r\n",
      " 'd_inner_hid': 2048,\r\n",
      " 'd_model': 512,\r\n",
      " 'dropout': 0.1,\r\n",
      " 'eos_idx': 1,\r\n",
      " 'epoch': 50,\r\n",
      " 'eps': '1e-9',\r\n",
      " 'infer_batch_size': 50,\r\n",
      " 'init_from_params': 'trained_models/CWMT2021_step_345000/',\r\n",
      " 'label_smooth_eps': 0.1,\r\n",
      " 'learning_rate': 2.0,\r\n",
      " 'max_length': 256,\r\n",
      " 'max_out_len': 256,\r\n",
      " 'n_best': 1,\r\n",
      " 'n_head': 8,\r\n",
      " 'n_layer': 6,\r\n",
      " 'output_file': 'train_dev_test/predict.txt',\r\n",
      " 'pad_factor': 8,\r\n",
      " 'predict_file': 'train_dev_test/ccmt2019-news.zh2en.source_bpe',\r\n",
      " 'print_step': 10,\r\n",
      " 'random_seed': 'None',\r\n",
      " 'save_model': 'trained_models',\r\n",
      " 'save_step': 20,\r\n",
      " 'special_token': ['<s>', '<e>', '<unk>'],\r\n",
      " 'src_vocab_fpath': 'train_dev_test/vocab.ch.src',\r\n",
      " 'src_vocab_size': 10000,\r\n",
      " 'training_file': 'train_dev_test/train.ch.bpe,train_dev_test/train.en.bpe',\r\n",
      " 'trg_vocab_fpath': 'train_dev_test/vocab.en.tgt',\r\n",
      " 'trg_vocab_size': 10000,\r\n",
      " 'unk_idx': 2,\r\n",
      " 'use_gpu': True,\r\n",
      " 'validation_file': 'train_dev_test/dev.ch.bpe,train_dev_test/dev.en.bpe',\r\n",
      " 'warmup_steps': 8000,\r\n",
      " 'weight_sharing': False}\r\n"
     ]
    }
   ],
   "source": [
    "# 读入参数\n",
    "yaml_file = 'transformer.base.yaml'\n",
    "with open(yaml_file, 'rt') as f:\n",
    "    args = AttrDict(yaml.safe_load(f))\n",
    "    pprint(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-13T06:54:13.469687Z",
     "iopub.status.busy": "2024-01-13T06:54:13.469000Z",
     "iopub.status.idle": "2024-01-13T07:06:46.026920Z",
     "shell.execute_reply": "2024-01-13T07:06:46.025763Z",
     "shell.execute_reply.started": "2024-01-13T06:54:13.469646Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-13 14:54:13,881] [    INFO] - step_idx: 0, epoch: 0, batch: 0, avg loss: 10.525280,  ppl: 37245.261719 \r\n",
      "[2024-01-13 14:54:14,986] [    INFO] - step_idx: 9, epoch: 0, batch: 9, avg loss: 10.511203,  ppl: 36724.625000 \r\n",
      "[2024-01-13 14:54:16,280] [    INFO] - step_idx: 19, epoch: 0, batch: 19, avg loss: 10.476138,  ppl: 35459.203125 \r\n",
      "[2024-01-13 14:54:16,392] [    INFO] - validation, step_idx: 19, avg loss: 10.481450,  ppl: 35648.062500\r\n",
      "[2024-01-13 14:54:29,635] [    INFO] - step_idx: 29, epoch: 1, batch: 9, avg loss: 10.414930,  ppl: 33353.914062 \r\n",
      "[2024-01-13 14:54:30,940] [    INFO] - step_idx: 39, epoch: 1, batch: 19, avg loss: 10.366558,  ppl: 31778.908203 \r\n",
      "[2024-01-13 14:54:31,067] [    INFO] - validation, step_idx: 39, avg loss: 10.387623,  ppl: 32455.423828\r\n",
      "[2024-01-13 14:54:44,980] [    INFO] - step_idx: 49, epoch: 2, batch: 9, avg loss: 10.295083,  ppl: 29586.783203 \r\n",
      "[2024-01-13 14:54:46,769] [    INFO] - step_idx: 59, epoch: 2, batch: 19, avg loss: 10.218628,  ppl: 27409.031250 \r\n",
      "[2024-01-13 14:54:46,939] [    INFO] - validation, step_idx: 59, avg loss: 10.286945,  ppl: 29346.992188\r\n",
      "[2024-01-13 14:55:00,222] [    INFO] - step_idx: 69, epoch: 3, batch: 9, avg loss: 10.180264,  ppl: 26377.416016 \r\n",
      "[2024-01-13 14:55:01,526] [    INFO] - step_idx: 79, epoch: 3, batch: 19, avg loss: 10.093046,  ppl: 24174.318359 \r\n",
      "[2024-01-13 14:55:01,645] [    INFO] - validation, step_idx: 79, avg loss: 10.165415,  ppl: 25988.640625\r\n",
      "[2024-01-13 14:55:15,054] [    INFO] - step_idx: 89, epoch: 4, batch: 9, avg loss: 10.028255,  ppl: 22657.710938 \r\n",
      "[2024-01-13 14:55:16,925] [    INFO] - step_idx: 99, epoch: 4, batch: 19, avg loss: 9.953676,  ppl: 21029.388672 \r\n",
      "[2024-01-13 14:55:17,095] [    INFO] - validation, step_idx: 99, avg loss: 10.014933,  ppl: 22357.847656\r\n",
      "[2024-01-13 14:55:31,013] [    INFO] - step_idx: 109, epoch: 5, batch: 9, avg loss: 9.843699,  ppl: 18839.281250 \r\n",
      "[2024-01-13 14:55:32,310] [    INFO] - step_idx: 119, epoch: 5, batch: 19, avg loss: 9.733507,  ppl: 16873.626953 \r\n",
      "[2024-01-13 14:55:32,431] [    INFO] - validation, step_idx: 119, avg loss: 9.834635,  ppl: 18669.281250\r\n",
      "[2024-01-13 14:55:45,826] [    INFO] - step_idx: 129, epoch: 6, batch: 9, avg loss: 9.618406,  ppl: 15039.063477 \r\n",
      "[2024-01-13 14:55:47,136] [    INFO] - step_idx: 139, epoch: 6, batch: 19, avg loss: 9.547073,  ppl: 14003.651367 \r\n",
      "[2024-01-13 14:55:47,276] [    INFO] - validation, step_idx: 139, avg loss: 9.628138,  ppl: 15186.127930\r\n",
      "[2024-01-13 14:56:00,410] [    INFO] - step_idx: 149, epoch: 7, batch: 9, avg loss: 9.450337,  ppl: 12712.454102 \r\n",
      "[2024-01-13 14:56:01,678] [    INFO] - step_idx: 159, epoch: 7, batch: 19, avg loss: 9.310778,  ppl: 11056.542969 \r\n",
      "[2024-01-13 14:56:01,795] [    INFO] - validation, step_idx: 159, avg loss: 9.401148,  ppl: 12102.263672\r\n",
      "[2024-01-13 14:56:14,862] [    INFO] - step_idx: 169, epoch: 8, batch: 9, avg loss: 9.199431,  ppl: 9891.503906 \r\n",
      "[2024-01-13 14:56:16,142] [    INFO] - step_idx: 179, epoch: 8, batch: 19, avg loss: 9.059095,  ppl: 8596.370117 \r\n",
      "[2024-01-13 14:56:16,256] [    INFO] - validation, step_idx: 179, avg loss: 9.165275,  ppl: 9559.346680\r\n",
      "[2024-01-13 14:56:29,670] [    INFO] - step_idx: 189, epoch: 9, batch: 9, avg loss: 8.952885,  ppl: 7730.158691 \r\n",
      "[2024-01-13 14:56:31,124] [    INFO] - step_idx: 199, epoch: 9, batch: 19, avg loss: 8.844887,  ppl: 6938.818359 \r\n",
      "[2024-01-13 14:56:31,268] [    INFO] - validation, step_idx: 199, avg loss: 8.930197,  ppl: 7556.751953\r\n",
      "[2024-01-13 14:56:44,965] [    INFO] - step_idx: 209, epoch: 10, batch: 9, avg loss: 8.717822,  ppl: 6110.855469 \r\n",
      "[2024-01-13 14:56:46,282] [    INFO] - step_idx: 219, epoch: 10, batch: 19, avg loss: 8.599040,  ppl: 5426.447754 \r\n",
      "[2024-01-13 14:56:46,392] [    INFO] - validation, step_idx: 219, avg loss: 8.704672,  ppl: 6031.022461\r\n",
      "[2024-01-13 14:56:59,613] [    INFO] - step_idx: 229, epoch: 11, batch: 9, avg loss: 8.470829,  ppl: 4773.471191 \r\n",
      "[2024-01-13 14:57:01,298] [    INFO] - step_idx: 239, epoch: 11, batch: 19, avg loss: 8.403822,  ppl: 4464.096191 \r\n",
      "[2024-01-13 14:57:01,453] [    INFO] - validation, step_idx: 239, avg loss: 8.496836,  ppl: 4899.241699\r\n",
      "[2024-01-13 14:57:15,791] [    INFO] - step_idx: 249, epoch: 12, batch: 9, avg loss: 8.216606,  ppl: 3701.917236 \r\n",
      "[2024-01-13 14:57:17,096] [    INFO] - step_idx: 259, epoch: 12, batch: 19, avg loss: 8.160786,  ppl: 3500.936279 \r\n",
      "[2024-01-13 14:57:17,220] [    INFO] - validation, step_idx: 259, avg loss: 8.311970,  ppl: 4072.326660\r\n",
      "[2024-01-13 14:57:30,904] [    INFO] - step_idx: 269, epoch: 13, batch: 9, avg loss: 8.098667,  ppl: 3290.079834 \r\n",
      "[2024-01-13 14:57:32,191] [    INFO] - step_idx: 279, epoch: 13, batch: 19, avg loss: 8.008337,  ppl: 3005.914307 \r\n",
      "[2024-01-13 14:57:32,302] [    INFO] - validation, step_idx: 279, avg loss: 8.158280,  ppl: 3492.176025\r\n",
      "[2024-01-13 14:57:45,778] [    INFO] - step_idx: 289, epoch: 14, batch: 9, avg loss: 7.905757,  ppl: 2712.855469 \r\n",
      "[2024-01-13 14:57:47,033] [    INFO] - step_idx: 299, epoch: 14, batch: 19, avg loss: 7.821897,  ppl: 2494.632080 \r\n",
      "[2024-01-13 14:57:47,236] [    INFO] - validation, step_idx: 299, avg loss: 8.034443,  ppl: 3085.419434\r\n",
      "[2024-01-13 14:58:00,690] [    INFO] - step_idx: 309, epoch: 15, batch: 9, avg loss: 7.815513,  ppl: 2478.757080 \r\n",
      "[2024-01-13 14:58:01,997] [    INFO] - step_idx: 319, epoch: 15, batch: 19, avg loss: 7.748292,  ppl: 2317.610596 \r\n",
      "[2024-01-13 14:58:02,110] [    INFO] - validation, step_idx: 319, avg loss: 7.943963,  ppl: 2818.507080\r\n",
      "[2024-01-13 14:58:15,838] [    INFO] - step_idx: 329, epoch: 16, batch: 9, avg loss: 7.658378,  ppl: 2118.318115 \r\n",
      "[2024-01-13 14:58:17,096] [    INFO] - step_idx: 339, epoch: 16, batch: 19, avg loss: 7.688163,  ppl: 2182.361328 \r\n",
      "[2024-01-13 14:58:17,215] [    INFO] - validation, step_idx: 339, avg loss: 7.881172,  ppl: 2646.973633\r\n",
      "[2024-01-13 14:58:30,690] [    INFO] - step_idx: 349, epoch: 17, batch: 9, avg loss: 7.611744,  ppl: 2021.801880 \r\n",
      "[2024-01-13 14:58:31,978] [    INFO] - step_idx: 359, epoch: 17, batch: 19, avg loss: 7.544521,  ppl: 1890.356812 \r\n",
      "[2024-01-13 14:58:32,092] [    INFO] - validation, step_idx: 359, avg loss: 7.843729,  ppl: 2549.695068\r\n",
      "[2024-01-13 14:58:45,683] [    INFO] - step_idx: 369, epoch: 18, batch: 9, avg loss: 7.560498,  ppl: 1920.801270 \r\n",
      "[2024-01-13 14:58:46,938] [    INFO] - step_idx: 379, epoch: 18, batch: 19, avg loss: 7.493549,  ppl: 1796.416016 \r\n",
      "[2024-01-13 14:58:47,056] [    INFO] - validation, step_idx: 379, avg loss: 7.821625,  ppl: 2493.955322\r\n",
      "[2024-01-13 14:59:00,532] [    INFO] - step_idx: 389, epoch: 19, batch: 9, avg loss: 7.559792,  ppl: 1919.445312 \r\n",
      "[2024-01-13 14:59:01,821] [    INFO] - step_idx: 399, epoch: 19, batch: 19, avg loss: 7.605562,  ppl: 2009.340332 \r\n",
      "[2024-01-13 14:59:01,964] [    INFO] - validation, step_idx: 399, avg loss: 7.804998,  ppl: 2452.831787\r\n",
      "[2024-01-13 14:59:15,944] [    INFO] - step_idx: 409, epoch: 20, batch: 9, avg loss: 7.491835,  ppl: 1793.340088 \r\n",
      "[2024-01-13 14:59:17,235] [    INFO] - step_idx: 419, epoch: 20, batch: 19, avg loss: 7.511627,  ppl: 1829.187622 \r\n",
      "[2024-01-13 14:59:17,349] [    INFO] - validation, step_idx: 419, avg loss: 7.793753,  ppl: 2425.402344\r\n",
      "[2024-01-13 14:59:30,499] [    INFO] - step_idx: 429, epoch: 21, batch: 9, avg loss: 7.423670,  ppl: 1675.169800 \r\n",
      "[2024-01-13 14:59:31,776] [    INFO] - step_idx: 439, epoch: 21, batch: 19, avg loss: 7.460293,  ppl: 1737.657593 \r\n",
      "[2024-01-13 14:59:31,897] [    INFO] - validation, step_idx: 439, avg loss: 7.775177,  ppl: 2380.763428\r\n",
      "[2024-01-13 14:59:45,612] [    INFO] - step_idx: 449, epoch: 22, batch: 9, avg loss: 7.465638,  ppl: 1746.970093 \r\n",
      "[2024-01-13 14:59:47,234] [    INFO] - step_idx: 459, epoch: 22, batch: 19, avg loss: 7.426867,  ppl: 1680.535034 \r\n",
      "[2024-01-13 14:59:47,418] [    INFO] - validation, step_idx: 459, avg loss: 7.770258,  ppl: 2369.083496\r\n",
      "[2024-01-13 15:00:00,837] [    INFO] - step_idx: 469, epoch: 23, batch: 9, avg loss: 7.423018,  ppl: 1674.078979 \r\n",
      "[2024-01-13 15:00:02,129] [    INFO] - step_idx: 479, epoch: 23, batch: 19, avg loss: 7.388788,  ppl: 1617.744507 \r\n",
      "[2024-01-13 15:00:02,248] [    INFO] - validation, step_idx: 479, avg loss: 7.743794,  ppl: 2307.210449\r\n",
      "[2024-01-13 15:00:15,476] [    INFO] - step_idx: 489, epoch: 24, batch: 9, avg loss: 7.349937,  ppl: 1556.098389 \r\n",
      "[2024-01-13 15:00:16,743] [    INFO] - step_idx: 499, epoch: 24, batch: 19, avg loss: 7.354565,  ppl: 1563.316284 \r\n",
      "[2024-01-13 15:00:16,861] [    INFO] - validation, step_idx: 499, avg loss: 7.723325,  ppl: 2260.463623\r\n",
      "[2024-01-13 15:00:30,003] [    INFO] - step_idx: 509, epoch: 25, batch: 9, avg loss: 7.272244,  ppl: 1439.778320 \r\n",
      "[2024-01-13 15:00:31,309] [    INFO] - step_idx: 519, epoch: 25, batch: 19, avg loss: 7.185494,  ppl: 1320.141846 \r\n",
      "[2024-01-13 15:00:31,423] [    INFO] - validation, step_idx: 519, avg loss: 7.708853,  ppl: 2227.985840\r\n",
      "[2024-01-13 15:00:44,464] [    INFO] - step_idx: 529, epoch: 26, batch: 9, avg loss: 7.112409,  ppl: 1227.100220 \r\n",
      "[2024-01-13 15:00:46,096] [    INFO] - step_idx: 539, epoch: 26, batch: 19, avg loss: 7.255960,  ppl: 1416.521484 \r\n",
      "[2024-01-13 15:00:46,214] [    INFO] - validation, step_idx: 539, avg loss: 7.685845,  ppl: 2177.309082\r\n",
      "[2024-01-13 15:00:59,092] [    INFO] - step_idx: 549, epoch: 27, batch: 9, avg loss: 7.130878,  ppl: 1249.974487 \r\n",
      "[2024-01-13 15:01:00,423] [    INFO] - step_idx: 559, epoch: 27, batch: 19, avg loss: 7.129540,  ppl: 1248.302490 \r\n",
      "[2024-01-13 15:01:00,592] [    INFO] - validation, step_idx: 559, avg loss: 7.663506,  ppl: 2129.209473\r\n",
      "[2024-01-13 15:01:13,503] [    INFO] - step_idx: 569, epoch: 28, batch: 9, avg loss: 7.094213,  ppl: 1204.973145 \r\n",
      "[2024-01-13 15:01:14,770] [    INFO] - step_idx: 579, epoch: 28, batch: 19, avg loss: 7.098202,  ppl: 1209.790283 \r\n",
      "[2024-01-13 15:01:14,902] [    INFO] - validation, step_idx: 579, avg loss: 7.648228,  ppl: 2096.927002\r\n",
      "[2024-01-13 15:01:28,021] [    INFO] - step_idx: 589, epoch: 29, batch: 9, avg loss: 6.981422,  ppl: 1076.447876 \r\n",
      "[2024-01-13 15:01:29,763] [    INFO] - step_idx: 599, epoch: 29, batch: 19, avg loss: 7.053621,  ppl: 1157.041138 \r\n",
      "[2024-01-13 15:01:29,936] [    INFO] - validation, step_idx: 599, avg loss: 7.625872,  ppl: 2050.568115\r\n",
      "[2024-01-13 15:01:44,401] [    INFO] - step_idx: 609, epoch: 30, batch: 9, avg loss: 6.832983,  ppl: 927.954712 \r\n",
      "[2024-01-13 15:01:45,701] [    INFO] - step_idx: 619, epoch: 30, batch: 19, avg loss: 6.978657,  ppl: 1073.475952 \r\n",
      "[2024-01-13 15:01:45,835] [    INFO] - validation, step_idx: 619, avg loss: 7.598839,  ppl: 1995.877930\r\n",
      "[2024-01-13 15:01:59,432] [    INFO] - step_idx: 629, epoch: 31, batch: 9, avg loss: 6.853087,  ppl: 946.799072 \r\n",
      "[2024-01-13 15:02:00,936] [    INFO] - step_idx: 639, epoch: 31, batch: 19, avg loss: 6.784035,  ppl: 883.627075 \r\n",
      "[2024-01-13 15:02:01,067] [    INFO] - validation, step_idx: 639, avg loss: 7.581195,  ppl: 1960.971680\r\n",
      "[2024-01-13 15:02:14,393] [    INFO] - step_idx: 649, epoch: 32, batch: 9, avg loss: 6.737445,  ppl: 843.403320 \r\n",
      "[2024-01-13 15:02:15,742] [    INFO] - step_idx: 659, epoch: 32, batch: 19, avg loss: 6.729410,  ppl: 836.653687 \r\n",
      "[2024-01-13 15:02:15,893] [    INFO] - validation, step_idx: 659, avg loss: 7.580992,  ppl: 1960.573486\r\n",
      "[2024-01-13 15:02:28,693] [    INFO] - step_idx: 669, epoch: 33, batch: 9, avg loss: 6.635159,  ppl: 761.400513 \r\n",
      "[2024-01-13 15:02:29,980] [    INFO] - step_idx: 679, epoch: 33, batch: 19, avg loss: 6.620834,  ppl: 750.570984 \r\n",
      "[2024-01-13 15:02:30,140] [    INFO] - validation, step_idx: 679, avg loss: 7.544920,  ppl: 1891.111450\r\n",
      "[2024-01-13 15:02:44,066] [    INFO] - step_idx: 689, epoch: 34, batch: 9, avg loss: 6.643238,  ppl: 767.576050 \r\n",
      "[2024-01-13 15:02:45,382] [    INFO] - step_idx: 699, epoch: 34, batch: 19, avg loss: 6.504543,  ppl: 668.170532 \r\n",
      "[2024-01-13 15:02:45,513] [    INFO] - validation, step_idx: 699, avg loss: 7.516392,  ppl: 1837.923584\r\n",
      "[2024-01-13 15:02:58,394] [    INFO] - step_idx: 709, epoch: 35, batch: 9, avg loss: 6.432563,  ppl: 621.765686 \r\n",
      "[2024-01-13 15:02:59,637] [    INFO] - step_idx: 719, epoch: 35, batch: 19, avg loss: 6.385857,  ppl: 593.392822 \r\n",
      "[2024-01-13 15:02:59,772] [    INFO] - validation, step_idx: 719, avg loss: 7.500027,  ppl: 1808.090576\r\n",
      "[2024-01-13 15:03:12,798] [    INFO] - step_idx: 729, epoch: 36, batch: 9, avg loss: 6.316024,  ppl: 553.368591 \r\n",
      "[2024-01-13 15:03:14,326] [    INFO] - step_idx: 739, epoch: 36, batch: 19, avg loss: 6.347567,  ppl: 571.101318 \r\n",
      "[2024-01-13 15:03:14,518] [    INFO] - validation, step_idx: 739, avg loss: 7.477291,  ppl: 1767.445557\r\n",
      "[2024-01-13 15:03:27,778] [    INFO] - step_idx: 749, epoch: 37, batch: 9, avg loss: 6.252088,  ppl: 519.095337 \r\n",
      "[2024-01-13 15:03:29,093] [    INFO] - step_idx: 759, epoch: 37, batch: 19, avg loss: 6.205299,  ppl: 495.367249 \r\n",
      "[2024-01-13 15:03:29,223] [    INFO] - validation, step_idx: 759, avg loss: 7.492095,  ppl: 1793.806885\r\n",
      "[2024-01-13 15:03:42,376] [    INFO] - step_idx: 769, epoch: 38, batch: 9, avg loss: 6.159319,  ppl: 473.105774 \r\n",
      "[2024-01-13 15:03:43,655] [    INFO] - step_idx: 779, epoch: 38, batch: 19, avg loss: 6.110506,  ppl: 450.566467 \r\n",
      "[2024-01-13 15:03:43,775] [    INFO] - validation, step_idx: 779, avg loss: 7.482320,  ppl: 1776.356812\r\n",
      "[2024-01-13 15:03:56,772] [    INFO] - step_idx: 789, epoch: 39, batch: 9, avg loss: 5.978234,  ppl: 394.742584 \r\n",
      "[2024-01-13 15:03:58,088] [    INFO] - step_idx: 799, epoch: 39, batch: 19, avg loss: 6.010865,  ppl: 407.835846 \r\n",
      "[2024-01-13 15:03:58,213] [    INFO] - validation, step_idx: 799, avg loss: 7.421112,  ppl: 1670.889771\r\n",
      "[2024-01-13 15:04:10,891] [    INFO] - step_idx: 809, epoch: 40, batch: 9, avg loss: 5.935128,  ppl: 378.088470 \r\n",
      "[2024-01-13 15:04:12,135] [    INFO] - step_idx: 819, epoch: 40, batch: 19, avg loss: 5.934764,  ppl: 377.950928 \r\n",
      "[2024-01-13 15:04:12,253] [    INFO] - validation, step_idx: 819, avg loss: 7.436334,  ppl: 1696.518677\r\n",
      "[2024-01-13 15:04:25,001] [    INFO] - step_idx: 829, epoch: 41, batch: 9, avg loss: 5.705768,  ppl: 300.596283 \r\n",
      "[2024-01-13 15:04:26,307] [    INFO] - step_idx: 839, epoch: 41, batch: 19, avg loss: 5.819122,  ppl: 336.676270 \r\n",
      "[2024-01-13 15:04:26,423] [    INFO] - validation, step_idx: 839, avg loss: 7.453074,  ppl: 1725.158081\r\n",
      "[2024-01-13 15:04:39,889] [    INFO] - step_idx: 849, epoch: 42, batch: 9, avg loss: 5.751238,  ppl: 314.579834 \r\n",
      "[2024-01-13 15:04:41,178] [    INFO] - step_idx: 859, epoch: 42, batch: 19, avg loss: 5.625557,  ppl: 277.426880 \r\n",
      "[2024-01-13 15:04:41,292] [    INFO] - validation, step_idx: 859, avg loss: 7.429104,  ppl: 1684.298218\r\n",
      "[2024-01-13 15:04:54,861] [    INFO] - step_idx: 869, epoch: 43, batch: 9, avg loss: 5.528277,  ppl: 251.709839 \r\n",
      "[2024-01-13 15:04:56,116] [    INFO] - step_idx: 879, epoch: 43, batch: 19, avg loss: 5.567704,  ppl: 261.832306 \r\n",
      "[2024-01-13 15:04:56,225] [    INFO] - validation, step_idx: 879, avg loss: 7.454597,  ppl: 1727.786621\r\n",
      "[2024-01-13 15:05:08,786] [    INFO] - step_idx: 889, epoch: 44, batch: 9, avg loss: 5.466303,  ppl: 236.583893 \r\n",
      "[2024-01-13 15:05:10,050] [    INFO] - step_idx: 899, epoch: 44, batch: 19, avg loss: 5.457896,  ppl: 234.603348 \r\n",
      "[2024-01-13 15:05:10,163] [    INFO] - validation, step_idx: 899, avg loss: 7.435758,  ppl: 1695.542725\r\n",
      "[2024-01-13 15:05:22,806] [    INFO] - step_idx: 909, epoch: 45, batch: 9, avg loss: 5.418025,  ppl: 225.433350 \r\n",
      "[2024-01-13 15:05:24,026] [    INFO] - step_idx: 919, epoch: 45, batch: 19, avg loss: 5.298318,  ppl: 200.000122 \r\n",
      "[2024-01-13 15:05:24,143] [    INFO] - validation, step_idx: 919, avg loss: 7.450842,  ppl: 1721.312500\r\n",
      "[2024-01-13 15:05:37,352] [    INFO] - step_idx: 929, epoch: 46, batch: 9, avg loss: 5.213546,  ppl: 183.744507 \r\n",
      "[2024-01-13 15:05:38,655] [    INFO] - step_idx: 939, epoch: 46, batch: 19, avg loss: 5.231404,  ppl: 187.055298 \r\n",
      "[2024-01-13 15:05:38,785] [    INFO] - validation, step_idx: 939, avg loss: 7.486119,  ppl: 1783.118774\r\n",
      "[2024-01-13 15:05:51,700] [    INFO] - step_idx: 949, epoch: 47, batch: 9, avg loss: 5.051752,  ppl: 156.296066 \r\n",
      "[2024-01-13 15:05:52,947] [    INFO] - step_idx: 959, epoch: 47, batch: 19, avg loss: 5.089955,  ppl: 162.382599 \r\n",
      "[2024-01-13 15:05:53,077] [    INFO] - validation, step_idx: 959, avg loss: 7.454329,  ppl: 1727.323608\r\n",
      "[2024-01-13 15:06:05,847] [    INFO] - step_idx: 969, epoch: 48, batch: 9, avg loss: 4.997787,  ppl: 148.085007 \r\n",
      "[2024-01-13 15:06:07,149] [    INFO] - step_idx: 979, epoch: 48, batch: 19, avg loss: 5.003536,  ppl: 148.938843 \r\n",
      "[2024-01-13 15:06:07,265] [    INFO] - validation, step_idx: 979, avg loss: 7.465931,  ppl: 1747.481567\r\n",
      "[2024-01-13 15:06:20,709] [    INFO] - step_idx: 989, epoch: 49, batch: 9, avg loss: 4.804832,  ppl: 122.098923 \r\n",
      "[2024-01-13 15:06:22,771] [    INFO] - step_idx: 999, epoch: 49, batch: 19, avg loss: 4.819923,  ppl: 123.955589 \r\n",
      "[2024-01-13 15:06:22,943] [    INFO] - validation, step_idx: 999, avg loss: 7.485377,  ppl: 1781.795532\r\n"
     ]
    }
   ],
   "source": [
    "do_train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 预测和评估\n",
    "模型最终训练的效果一般可通过测试集来进行测试，机器翻译领域一般计算BLEU值。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/cf7161d1059e4ca9989e03fc09f197c63590185720de444fa7c2d15ac3bee696\" width=\"600\" height=\"300\" ></center>\n",
    "<br><center>图11： 预测和评估 </center></br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-13T07:20:00.648574Z",
     "iopub.status.busy": "2024-01-13T07:20:00.647812Z",
     "iopub.status.idle": "2024-01-13T07:20:00.658974Z",
     "shell.execute_reply": "2024-01-13T07:20:00.658136Z",
     "shell.execute_reply.started": "2024-01-13T07:20:00.648534Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def do_predict(args):\n",
    "    if args.use_gpu:\n",
    "        place = \"gpu\"\n",
    "    else:\n",
    "        place = \"cpu\"\n",
    "    paddle.set_device(place)\n",
    "\n",
    "    # Define data loader\n",
    "    test_loader, to_tokens = create_infer_loader(args)\n",
    "\n",
    "    # Define model\n",
    "    transformer = InferTransformer(\n",
    "        src_vocab_size=args.src_vocab_size,\n",
    "        trg_vocab_size=args.trg_vocab_size,\n",
    "        max_length=args.max_length + 1,\n",
    "        num_encoder_layers=args.n_layer,\n",
    "        num_decoder_layers=args.n_layer,\n",
    "        n_head=args.n_head,\n",
    "        d_model=args.d_model,\n",
    "        d_inner_hid=args.d_inner_hid,\n",
    "        dropout=args.dropout,\n",
    "        weight_sharing=args.weight_sharing,\n",
    "        bos_id=args.bos_idx,\n",
    "        eos_id=args.eos_idx,\n",
    "        beam_size=args.beam_size,\n",
    "        max_out_len=args.max_out_len)\n",
    "\n",
    "    # Load the trained model\n",
    "    assert args.init_from_params, (\n",
    "        \"Please set init_from_params to load the infer model.\")\n",
    "\n",
    "    model_dict = paddle.load(\n",
    "        os.path.join(args.init_from_params, \"transformer.pdparams\"))\n",
    "\n",
    "    # To avoid a longer length than training, reset the size of position\n",
    "    # encoding to max_length\n",
    "    model_dict[\"encoder.pos_encoder.weight\"] = position_encoding_init(\n",
    "        args.max_length + 1, args.d_model)\n",
    "    model_dict[\"decoder.pos_encoder.weight\"] = position_encoding_init(\n",
    "        args.max_length + 1, args.d_model)\n",
    "    transformer.load_dict(model_dict)\n",
    "\n",
    "    # Set evaluate mode\n",
    "    transformer.eval()\n",
    "\n",
    "    f = open(args.output_file, \"w\")\n",
    "    with paddle.no_grad():\n",
    "        for (src_word, ) in test_loader:\n",
    "            finished_seq = transformer(src_word=src_word)\n",
    "            finished_seq = finished_seq.numpy().transpose([0, 2, 1])\n",
    "            for ins in finished_seq:\n",
    "                for beam_idx, beam in enumerate(ins):\n",
    "                    if beam_idx >= args.n_best:\n",
    "                        break\n",
    "                    id_list = post_process_seq(beam, args.bos_idx, args.eos_idx)\n",
    "                    word_list = to_tokens(id_list)\n",
    "                    sequence = \" \".join(word_list) + \"\\n\"\n",
    "                    f.write(sequence)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-13T07:20:03.286262Z",
     "iopub.status.busy": "2024-01-13T07:20:03.284939Z",
     "iopub.status.idle": "2024-01-13T07:21:47.990434Z",
     "shell.execute_reply": "2024-01-13T07:21:47.989283Z",
     "shell.execute_reply.started": "2024-01-13T07:20:03.286218Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "do_predict(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型评估\n",
    "预测结果中每行输出是对应行输入的得分最高的翻译，对于使用 BPE 的数据，预测出的翻译结果也将是 BPE 表示的数据，要还原成原始的数据（这里指 tokenize 后的数据）才能进行正确的评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-13T07:22:52.291858Z",
     "iopub.status.busy": "2024-01-13T07:22:52.291226Z",
     "iopub.status.idle": "2024-01-13T07:22:55.764614Z",
     "shell.execute_reply": "2024-01-13T07:22:55.762849Z",
     "shell.execute_reply.started": "2024-01-13T07:22:52.291815Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 38.11, 74.5/49.1/32.5/21.7 (BP=0.951, ratio=0.952, hyp_len=22252, ref_len=23371)\r\n",
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\r\n"
     ]
    }
   ],
   "source": [
    "# 还原 predict.txt 中的预测结果为 tokenize 后的数据\n",
    "! sed -r 's/(@@ )|(@@ ?$)//g' train_dev_test/predict.txt > train_dev_test/predict.tok.txt\n",
    "# BLEU评估工具来源于 https://github.com/moses-smt/mosesdecoder.git\n",
    "! tar -zxf mosesdecoder.tar.gz\n",
    "# 计算multi-bleu\n",
    "! perl mosesdecoder/scripts/generic/multi-bleu.perl train_dev_test/ccmt2019-news.zh2en.ref*.txt < train_dev_test/predict.tok.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
